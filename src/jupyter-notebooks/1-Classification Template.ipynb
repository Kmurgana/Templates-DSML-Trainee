{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52924f90",
   "metadata": {},
   "source": [
    "# **<span style=\"color:#0D47A1;\">SELECCIONAR TEXTO Y COLOR </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d24b8",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Kmurgana/Resources-Random/main/Images/CAIXABANK.jpg\">\n",
    "\n",
    "> Descripci√≥n b√°sica\n",
    "> \n",
    "> \n",
    "> [HACKATHON DE XXXXXX](https://P√ÅGINA)\n",
    "> \n",
    "> - To get the API key, create an account in the [website](https://wandb.ai/site) .\n",
    "> - Use secrets to use API Keys more securely "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b35a3d",
   "metadata": {},
   "source": [
    "# **<span style=\"color:#0D47A1;\">Objetivos</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c7540",
   "metadata": {},
   "source": [
    "El reto consistir√° en modelar un peque√±o algoritmo predictivo que permita conocer las futuras din√°micas del mercado a partir de los datos hist√≥ricos del valor del IBEX35 y de algunos Tweets\n",
    "\n",
    "**Task**\n",
    "\n",
    "Create a model to predict the pollutant variable in the test_x dataset.\n",
    "Create a presentation (MAX 4 slides) explaining what you have done and why you have done it.. (Example: Download example )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957a9e5",
   "metadata": {},
   "source": [
    "**Roadmap**\n",
    "\n",
    "Before we dive headlong into programming, we should outline a brief guide to keep us on track. The following steps form the basis of any machine learning workflow once we have a problem and a model in mind:\n",
    "\n",
    "* 1) Raising the question.\n",
    "* 2) Acquiring the data. \n",
    "* 3) Adapting information, identifying and correcting anomalies (EDA).\n",
    "* 4) Preparar los datos para el modelo de aprendizaje autom√°tico\n",
    "* 5) Entrenar el modelo con los datos de entrenamiento\n",
    "* 6) Hacer predicciones con los datos de prueba\n",
    "* 7) Comparar las predicciones con los objetivos conocidos del conjunto de pruebas y calcular las m√©tricas de rendimiento\n",
    "* 8) Si el rendimiento no es satisfactorio, ajustar el modelo, adquiera m√°s datos o pruebe otra t√©cnica de modelizaci√≥n.\n",
    "* 9) Interpretar el modelo y comunicar los resultados de forma visual y num√©rica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b89320",
   "metadata": {},
   "source": [
    "## ‚úÖ1) Raising the question  ‚ùì\n",
    "\n",
    "The exercise consists of obtaining a predictive model capable of classifying whether the variable ...... is going to be ........., according to ........, for which it would be **objective** for the model to be as accurate as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27ccd6",
   "metadata": {},
   "source": [
    "## ‚úÖ 2) Acquiring the data  üîç\n",
    "\n",
    "### **<span style=\"color:#43A047;\">2.1 Importing required libraries. </span>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c6b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data acquirement and processing\n",
    "# ======================================================================================\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np \n",
    "import nltk\n",
    "import datetime as dt\n",
    "\n",
    "# Graphics\n",
    "# ======================================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing \n",
    "# ======================================================================================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP (Natural Language Programming) \n",
    "# ======================================================================================\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import pr\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Modeling for a categorical target\n",
    "# ======================================================================================\n",
    "from sklearn.linear_model import LogisticRegression   \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# Testing and validating\n",
    "# ======================================================================================\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import multiprocessing\n",
    "\n",
    "# Warnings Configuration\n",
    "# ======================================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9b150",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#43A047;\">2.2 Reading data from CSV </span>**\n",
    "\n",
    "#### **<span style=\"color:#4CAF50;\">2.2.1 Reading first data CSV </span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b720e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_csv_df = pd.read_csv(\"\", sep=',') #1-Loading the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_csv_df.head() #2-We look at the heading and check that it was read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9281259",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_csv_df.shape() #3-Quickly we check the format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681a5c4",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#43A047;\">2.3 Reading data from API json </span>**\n",
    "\n",
    "#### **<span style=\"color:#4CAF50;\">2.3.1. Reading first json data </span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://schneiderapihack-env.eba-3ais9akk.us-east-2.elasticbeanstalk.com/first'\n",
    "data1_js_df = pd.read_json(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_js_df = pd.read_csv(\"\", sep=',') #1-Loading the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f82463",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_js_df.head() #2-We look at the heading and check that it was read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_js_df.shape() #3-Quickly we check the format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c79fd",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#43A047;\">2.4 Reading data from PDF </span>**\n",
    "\n",
    "#### **<span style=\"color:#4CAF50;\">2.4.1. Reading first PDF data. </span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(pdf_path):\n",
    "    import PyPDF2\n",
    "    import tabula\n",
    "    import camelot\n",
    "    \"\"\"Function to exctract information from PDF's files\"\"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        pdf = PyPDF2.PdfFileReader(f)\n",
    "        information = pdf.getDocumentInfo()\n",
    "        number_of_pages = pdf.getNumPages()\n",
    "        page = pdf.pages[0]\n",
    "        text = page.extractText()\n",
    "        # WITH TABULA\n",
    "        #df = tabula.read_pdf(pdf_path, multiple_tables=True, pages=number_of_pages)\n",
    "        # WITH CAMELOT\n",
    "        df = camelot.read_pdf(pdf_path) \n",
    "        \n",
    "    txt = f\"\"\"\n",
    "    Information about {pdf_path}: \n",
    "    Author: {information.author}\n",
    "    Creator: {information.creator}\n",
    "    Producer: {information.producer}\n",
    "    Subject: {information.subject}\n",
    "    Title: {information.title}\n",
    "    Number of pages: {number_of_pages}\n",
    "    \"\"\"\n",
    "    print(txt)\n",
    "    \n",
    "    return information, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629eb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We execute the function as standa alone.\n",
    "if __name__ == '__main__':\n",
    "    path = \"../../../NUWE/Practice/3- Schneider Hackathon/Datasets/train6/pdfs-1.pdf\"\n",
    "    information, data3_pdf = extract_information(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cc817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3_df.head() #2-We look at the heading and check that it was read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f648223",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3_df.shape() #3-Quickly we check the format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a818d6",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#43A047;\">2.5 Reading data from TWITTERS  </span>**\n",
    "\n",
    "#### **<span style=\"color:#4CAF50;\">2.5.1. Reading twitters data sets. </span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b817ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "nltk.download('stopwords')\n",
    "stopword=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_twt_df = pd.read_excel(\"war_tweets.xls\") #1-Loading the excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1132d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_twt_df.head() #2-We look at the heading and check that it was read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f09b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_twt_df.shape() #3-Quickly we check the format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea28f2a",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd19a1",
   "metadata": {},
   "source": [
    "We remove boring tweets (tweets with only urls or too short) and cleanup texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_text(text):\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae080fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet, allow_new_lines = False):\n",
    "    bad_start = ['http:', 'https:']\n",
    "    for w in bad_start:\n",
    "        tweet = re.sub(f\" {w}\\\\S+\", \"\", tweet)      # removes white space before url\n",
    "        tweet = re.sub(f\"{w}\\\\S+ \", \"\", tweet)      # in case a tweet starts with a url\n",
    "        tweet = re.sub(f\"\\n{w}\\\\S+ \", \"\", tweet)    # in case the url is on a new line\n",
    "        tweet = re.sub(f\"\\n{w}\\\\S+\", \"\", tweet)     # in case the url is alone on a new line\n",
    "        tweet = re.sub(f\"{w}\\\\S+\", \"\", tweet)       # any other case?\n",
    "    tweet = re.sub(' +', ' ', tweet)                # replace multiple spaces with one space (makes the previous work worthless?)\n",
    "    if not allow_new_lines:                         # TODO: predictions seem better without new lines\n",
    "        tweet = ' '.join(tweet.split())\n",
    "    return tweet.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c32ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boring_tweet(tweet):\n",
    "    \"Check if this is a boring tweet\"\n",
    "    boring_stuff = ['http', '@', '#']\n",
    "    not_boring_words = len([None for w in tweet.split() if all(bs not in w.lower() for bs in boring_stuff)])\n",
    "    return not_boring_words < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_tweets = [fix_text(tweet) for tweet in my_tweets]\n",
    "clean_tweets = [clean_tweet(tweet) for tweet in curated_tweets]\n",
    "cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bc07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Curated tweets: {len(curated_tweets)}\\nCool tweets: {len(cool_tweets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cf84c2",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dba330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969804f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_extract(text_list):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for text in text_list:\n",
    "        ht = re.findall(r\"#(\\w+)\", text)\n",
    "        hashtags.append(ht)\n",
    "    return hashtags\n",
    "\n",
    "def generate_hashtag_freqdist(hashtags):\n",
    "    a = nltk.FreqDist(hashtags)\n",
    "    d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                    'Count': list(a.values())})\n",
    "    # selecting top 15 most frequent hashtags\n",
    "    d = d.nlargest(columns=\"Count\", n = 25)\n",
    "    plt.figure(figsize=(16,7))\n",
    "    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "    plt.xticks(rotation=80)\n",
    "    ax.set(ylabel = 'Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6600f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = hashtag_extract(data[\"tweet\"])\n",
    "hashtags = sum(hashtags, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56160bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_hashtag_freqdist(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_length_characters'] = data['tweet'].str.len()\n",
    "print(data['total_length_characters'])\n",
    "total_length_characters = data['total_length_characters'].sum()\n",
    "print(total_length_characters)\n",
    "count = 0\n",
    "for y in data[\"tweet\"]:\n",
    "    count = count + 1\n",
    "print(count)\n",
    "average_length = total_length_characters / count\n",
    "print (average_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf343bf",
   "metadata": {},
   "source": [
    "### 2.5 Merging all the information in a single data set.\n",
    "##### Concatenado de dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d431d0b",
   "metadata": {},
   "source": [
    "Concatenamos los csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3e533",
   "metadata": {},
   "source": [
    "## ‚úÖ  3) Adapting information, identifying and correcting anomalies üîß\n",
    "First, we have to merge all the datasets into one, so that we have a single package to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50c003",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#43A047;\">3.1 Checking column orders and differences in data sizes. </span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed718bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [[\"1st csv\", data1_csv_df.shape],\n",
    "         [\"1st json\", data1_json_df.shape],\n",
    "         [\"1st pdf\", data1_pdf_df.shape],\n",
    "         [\"1st twt\", data1_twt_df.shape]]\n",
    "print(\"The sizes are %s\" %sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900a568",
   "metadata": {},
   "source": [
    "If there is a difference in columns sizes, the differents columns should be displayed and determine what to do if it is vital information. You can either add a mean to the missing data or delete the extra columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d8f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [[\"1st csv\", data1_csv_df.columns],\n",
    "          [\"1st json\", data1_json_df.columns],\n",
    "          [\"1st pdf\", data1_pdf_df.columns],\n",
    "          [\"1st twt\", data1_twt_df.columns]]\n",
    "print(\"The order of the columns is %s\" %sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b070e10",
   "metadata": {},
   "source": [
    "If there is a different order, the data frame in question can be reindexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"C\", \"A\", \"B\"]\n",
    "df = df.reindex(columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0a1a3",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#43A047;\">3.2 Checking anomalies or outliers in all datasets.  </span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_checking (df, sd=2):\n",
    "    \"Autocheking for any Anomalies or Outliers\"\n",
    "    rows_problems = []\n",
    "    max_problems = []\n",
    "    min_problems = []\n",
    "    null_data = []\n",
    "    for i in range(0,len(data.describe().columns)): #This will check in numeric's columns\n",
    "        \n",
    "        if data.describe().iloc[0,i:i+1][0] != data.shape[0]: #Checking rows discordance \n",
    "            rows_problems.append(df.describe().columns[i])\n",
    "       \n",
    "        #Checking minimum data range\n",
    "        if int(df.describe().iloc[3,i:i+1][0]) < (int(df.describe().iloc[4,i:i+1][0]) - sd*(int(df.describe().iloc[2,i:i+1][0]))):  \n",
    "            min_problems.append(df.describe().columns[i])\n",
    "        \n",
    "        #Checking maximum data range\n",
    "        if int(data.describe().iloc[7,i:i+1]) > (int(data.describe().iloc[6,i:i+1]) + sd*int(df.describe().iloc[2,i:i+1][0])): \n",
    "            max_problems.append(df.describe().columns[i])\n",
    "    \n",
    "    for i in range (0, data.shape[1]): # This will check in the entire DataFrame   \n",
    "        \n",
    "        if (df.isnull().sum()[i] != 0) or (df.isna().sum()[i] != 0): #Checking null or nan datas\n",
    "            null_data.append(df.columns[i])        \n",
    "    print(\"Columns with rows discordance: %s\" %rows_problems)\n",
    "    print(\"Columns with Min. values problems: %s\" %min_problems)\n",
    "    print(\"Columns with Max. values problems: %s\" %max_problems)\n",
    "    print(\"Columns with NaN or Nulls: %s\" %null_data)\n",
    "    return rows_problems, min_problems, max_problems, null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57faffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_multiplier = 2 #Standar Deviation Multiplier to chek how far away the 25% and 75% outliers should be.\n",
    "row_prblm, min_col, max_col, null_col = data_checking (data, sd_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8269ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_1 = pd.concat([train1, train2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a22d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train1.shape[0]+train2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dee6d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ec9be",
   "metadata": {},
   "source": [
    "Concatenamos los json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7704a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_2 = pd.concat([train3, train4, train5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd241e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train3.shape[0]+train4.shape[0]+train5.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790211f",
   "metadata": {},
   "source": [
    "Concatenaci√≥n de los csv y json final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([merged_1, merged_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_1.shape[0]+merged_2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cfc70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7140175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of our features is:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc854ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0844707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee0685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3f7528e",
   "metadata": {},
   "source": [
    "### ‚úÖ  4) Preparar datos para training y testing üèÉ‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f32575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos librer√≠a para dividir aleatoriamente entre training y testing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.2)\n",
    "print(\"Longitud de datos de entrenamiento: %f\" %(len(train)), \"datos de testing: %f\" %(len(test)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asignaci√≥n de variables predictoras y target\n",
    "colnames = df.columns.values.tolist()\n",
    "predictors=colnames[1:xx]\n",
    "target=colnames[xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87ca13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23f7b0fc",
   "metadata": {},
   "source": [
    "### ‚úÖ 5) Entrenar el modelo con los datos de entrenamientoüèãÔ∏è‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def find_best_model_using_gridsearchcv(X,y):\n",
    "    \"\"\"Function with three models for classification\"\"\"\n",
    "    \n",
    "    algos = {\n",
    "        'Logistic_Regression' : {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'max_iter':[1000, 2000, 5000],\n",
    "            #'C':[,10,100,1000],\n",
    "            'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']  \n",
    "            }\n",
    "        },\n",
    "        'Linear_Discriminant_Analysis': {\n",
    "        'model': LinearDiscriminantAnalysis(),\n",
    "        'params': {\n",
    "            'solver':['svd', 'lsqr', 'eigen']  \n",
    "            }\n",
    "        },      \n",
    "        'K_Nearest_Neighbors':{\n",
    "            'model' : KNeighborsClassifier(),\n",
    "            'params':{\n",
    "            'n_neighbors': [5,10],\n",
    "            'metric': [\"minkowski\", 'euclidean', 'manhattan', 'chebyshev', 'wminkowski', 'seuclidean', 'mahalanobis'],\n",
    "            'p': [2],\n",
    "            'n_neighbors': range(100)\n",
    "            }                                                        \n",
    "        },\n",
    "        'Decision_Tree_Classifier': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'criterion': ['gini','entropy', 'log_loss'],\n",
    "            'min_samples_split': [10,100,1000],\n",
    "            'max_depth' :[3,5,10,20],\n",
    "            'splitter': ['best','random']\n",
    "            }\n",
    "        },\n",
    "        'Random_Forest_Classifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_jobs' : [multiprocessing.cpu_count() - 1],            \n",
    "            'n_estimators': [10,100,200,500],\n",
    "            'max_depth'   : [None, 3, 10, 20],\n",
    "            'criterion'   : ['gini', 'entropy']\n",
    "            }\n",
    "        },      \n",
    "        'MLP_Classifier': {\n",
    "        'model': MLPClassifier(),\n",
    "        'params': {            \n",
    "            'max_iter':[1000],\n",
    "            'alpha':[0.001,0.01,0.1,1],\n",
    "            }\n",
    "        },      \n",
    "        'Support_Vector_Classifier': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C':[1,100,1000,10000],\n",
    "            'gamma':[0.0001,0.001,0.01,0.1]\n",
    "            }\n",
    "         },       \n",
    "        #'Gradien_Boosting_Classifier': {\n",
    "        #'model': GradientBoostingClassifier(),\n",
    "        #'params':  {\n",
    "        #    'n_estimators'  : [50, 100, 500, 1000],\n",
    "        #    'max_features'  : ['auto', 'sqrt', 'log2'],\n",
    "        #    'max_depth'     : [None, 1, 3, 5, 10, 20],\n",
    "        #    'subsample'     : [0.5, 1],\n",
    "        #    'learning_rate' : [0.001, 0.01, 0.1]\n",
    "        #    }\n",
    "        #}         \n",
    "    }\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    for algo_name, config in algos.items():\n",
    "        gs = GridSearchCV(config['model'], config['params'], cv=cv, return_train_score = True)\n",
    "        gs.fit(X,y)\n",
    "        scores.append({\n",
    "            'model': algo_name,\n",
    "            'best_score': gs.best_score_,\n",
    "            'best_params': gs.best_params_\n",
    "        }) \n",
    "    return pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "best = find_best_model_using_gridsearchcv(X_train_s,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c772e5b",
   "metadata": {},
   "source": [
    "### ‚úÖ 6) Making predictions with test data ü§î¬∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_predict(location,sqft,bath,BHK):\n",
    "    loc_index=np.where(X.columns==location)[0][0]\n",
    "    x=np.zeros(len(X.columns))\n",
    "    x[0]=sqft\n",
    "    x[1]=bath\n",
    "    x[2]=BHK\n",
    "    if loc_index >=0:\n",
    "    x[loc_index]=1\n",
    "    return model.predict([x])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489850e8",
   "metadata": {},
   "source": [
    "### ‚úÖ 7) Comparing predictions with test dataüìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff13d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cbc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification_report(Y_test, y_pred, target_names=[\"xxxxx\", \"xxxxx\", \"xxxxx\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Homogeneidad: %0.3f\" %metrics.homogeneity_score(labels, clust_labels))\n",
    "#print(\"Completitud: %0.3f\" %metrics.completeness_score(labels, clust_labels))\n",
    "#print(\"V-measure: %0.3f\" %metrics.v_measure_score(labels, clust_labels))\n",
    "#print(\"R2 ajustado: %0.3f\" %metrics.adjusted_rand_score(labels, clust_labels))\n",
    "#print(\"Informaci√≥n m√∫tua ajustada: %0.3f\" %metrics.adjusted_mutual_info_score(labels,clust_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0a03a",
   "metadata": {},
   "source": [
    "### ‚úÖ 8) Adjusting the modelüî®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0f229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8388e931",
   "metadata": {},
   "source": [
    "### ‚úÖ 9) Submitting  results üåü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaeafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(pred, columns=['Results'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"../../Practice/1-Random Forest/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883aa15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
